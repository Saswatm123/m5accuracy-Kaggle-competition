---
title: "M5 Prediction Accuracy"
output: html_notebook
---

```{r}
library(tidyverse)
library(lubridate)
library(ggplot2)
```

```{r}
datadir = '../datasets/m5accuracy/'
data.sales = read.csv( paste(datadir, 'sales_train_validation.csv', sep = '') )
data.calendar = read.csv( paste(datadir, 'calendar.csv', sep = '') )
data.prices = read.csv( paste(datadir, 'sell_prices.csv', sep = '') )
```

```{r}
colnames(data.sales)[1:6]
```

```
ID is just concatenated version of other columns, so we can ignore that.
```
```{r}
data.sales %>%
  head(20)
```

```
data.sales:
  id:
    concat version of all other columns
  item_id:
    concat of dept_id and identifying number
  dept_id:
    concat of cat_id and identifying number
  cat_id:
    HOBBIES, HOUSEHOLD, FOODS
  store_id:
    concat of state_id and identifying number [1-4]
  state_id:
    CA, TX, WI
  d[1-X]:
    sales on days 1-X
```
```{r}
#SUMMARY: store_id is in the form "STATEABBR_[1-4]", 3049 of each. States are CA TX WI.

data.sales %>%
  select(store_id) %>%
  filter(str_detect(store_id, "[A-Z]{2}_\\d+") ) %>%
  table()
```

```
Now will see how each store performs over time
```
```{r}
perstore.timeline = data.sales %>%
  group_by(store_id) %>%
  summarize_at(vars(starts_with("d_")), sum )
perstore.timeline
```

```{r}
#we make a new data frame that makes the timeline easier to plot by transposing the perstore.timeline data frame

perstore.timeline.T = data.table::transpose(perstore.timeline[-1])
colnames(perstore.timeline.T) = perstore.timeline$store_id

perstore.timeline.T
```

```{r}
plot.timeline.perstore = perstore.timeline.T %>%
  gather(key = "store_id", value = "n_sales")
plot.timeline.perstore$daynum = rep(1:nrow(perstore.timeline.T), ncol(perstore.timeline.T) )
plot.timeline.perstore
```

```{r}
ggplot(plot.timeline.perstore, aes(x = daynum, y = n_sales, color = store_id) ) +
  geom_line() +
  facet_grid(~store_id)
```

```
This could be combined with weather data or other time data if we find the exact dates this was collected on. 

Each store has a unique pattern to it so store_id is a major feature.

The dip of 0 sales has a period of 365 days, with 366 days in between the first and second dips. The first year is a leap year if the closing date is before Feb 29th, and the second being a leap year if the closing date is after Feb 29th.
Edit: The closing date is Christmas each year.
Edit: All dates are provided in data.calendar along with their d_X equivalent

This also means that no days were skipped recording, and these are consecutive dates.
```
```{r}
#Looking at how many zero sale dates there are in all locations
print(lapply(perstore.timeline.T, function(x){ sum(x==0) } ) )

#We can see that there are 5 main dips in each store location, so possible that online purchases count as store sales. Let's instead look for very low sales
#less than some certain threshold until we get 5 for all of them, and 6 for WI_1
LOW_SALES_THRESH = 15
print(lapply(perstore.timeline.T, function(x){ sum(x<=LOW_SALES_THRESH) } ) )

#Looks like 15 works. Let's check the lag between these dates.
lapply(perstore.timeline.T, function(x){ which(x<=LOW_SALES_THRESH) } ) %>% 
  lapply(diff) %>%
  print()

#Perfect, lags are all 1 year apart perfectly. The only anomaly is WI_1 with its early low day. 
#Could have just been a store emergency closing but if I find out which store this is I can account for that.
#It is interesting that for WI_1, if the first dip is an anomaly, the real dip did not occur until 366 days after that time,
#so maybe looking at the info about the specific first low sales day for WI_1 would help.
print( which(perstore.timeline.T$WI_1 <= LOW_SALES_THRESH) )

#When we get to the specific dates, we can find out what the 5th day is and go from there.
```

```
Now we look at price data over the specific dates.
```
```{r}
data.prices %>%
  head(20)
``` 

```
Since the info here is time based as well, I'll look at the calendar file to see any more interesting time data and add/remove features accordingly.
```
```{r}
data.calendar %>%
  head(20)
```

```
data.sales: number of each item sold on each day along with location info (units per location/item/day)
data.prices:  price of each item sold on each day along with location info (price per location/item/day)
data.calendar:  specific dates along with corresponding day info

Lets look at the time info provided by data.calendar
```
```{r}
data.calendar = data.calendar %>%
  mutate(date = ymd(date),
         month = factor(month, levels = as.character(1:12) ),
         wday  = factor(wday, levels = as.character(1:12) ) )

#Pivot time columns into rows so that we can join with calendar's "d" column later
units.datewise.expanded = data.sales %>%
  select(item_id, store_id, cat_id, starts_with("d_") ) %>%
  pivot_longer(cols = starts_with("d_"), names_to = "d", values_to = "units" )

#Contains date to price per loc
prices.as.calendar = data.calendar %>%
  left_join(data.prices, by = "wm_yr_wk")

#Contains price & amount (volume), as well as all relevant details.
volume.details = prices.as.calendar %>%
  left_join(units.datewise.expanded, by = c("store_id" = "store_id", "item_id" = "item_id", "d" = "d") ) %>%
  filter(! (units %>% is.na) ) #This filter is for the store/item combinations that don't exist, which will return NA for sales.
```

```
volume.details is master data frame of volume, units, price vs all signal level combinations (time, state, etc.)
all sub-level data frames only need [date, signal_level, price, units, volume] + [additional features (SNAP, days till holiday etc)]
```
```{r}
colnames(volume.details)
```

```{r}
#Add volume col for easy volume analysis
volume.details = volume.details %>%
  mutate(volume = units * sell_price)
```

```
Volume, units, sell price vs date.
Gray is volume, Red is units, Blue is sell price
```
```{r}
volume.details %>%
  group_by(date) %>%
  summarize(volume = sum(volume), sell_price = sum(sell_price), units = sum(units) ) %>%
  ggplot(aes(x = date) ) +
  geom_path(aes(y = sell_price), color = "blue" ) +
  geom_path(aes(y = units), color = "red" ) +
  geom_path(aes(y = volume), color = "gray" )
```

```{r}
#Avg volume for weekend vs weekday
volume.details %>%
  group_by( weekday %in% c("Saturday", "Sunday") ) %>%
  summarize(meansales = mean(volume), medsales = median(volume) ) %>%
  print

#Mean for weekend is higher for weekend than weekday. Will add feature is.weekend
volume.details = volume.details %>%
  mutate(is.weekend = weekday %in% c("Saturday", "Sunday") ) %>%
  select(is.weekend, everything() ) #Moves the lsat column to the front.
```

```{r}
volume.details %>%
  group_by(date) %>%
  summarize(units = sum(units), price = sum(sell_price), volume = sum(units*sell_price) ) %>%
  ggplot(aes(x = date) ) +
    geom_path(aes(y = price ) )

#This is a useful dataframe, so I'll save it for later.
vol.by.date = volume.details %>%
  group_by(date) %>%
  summarize(units = sum(units), price = sum(sell_price), volume = sum(units*sell_price) )
```

```
Feature Aggregation levels:
  State -> Store ID
  Category ID -> Department ID -> Item ID

Target Aggregation Levels:
  Volume
  Price * Units
```
